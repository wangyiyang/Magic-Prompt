import os
import uuid
import time
import logging
from typing import List, Generator, Tuple, Any, Dict, Optional

import gradio as gr
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, ToolMessage
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("magic-prompt")
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from pydantic import BaseModel
from typing import Annotated
from typing_extensions import TypedDict, Literal
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize LLM
llm = ChatOpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_BASE_URL"),
    model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
    temperature=0,
)

# Define templates
template = """ä½ çš„èº«ä»½æ˜¯ä¸€ä¸ªæç¤ºè¯ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä»ç”¨æˆ·é‚£é‡Œè·å–æœ‰å…³ä»–ä»¬æƒ³è¦åˆ›å»ºçš„æç¤ºè¯æ¨¡æ¿ç±»å‹çš„ä¿¡æ¯ã€‚

ä½ åº”è¯¥è·å–ä»¥ä¸‹ä¿¡æ¯ï¼š

- æç¤ºè¯çš„ç›®æ ‡æ˜¯ä»€ä¹ˆ
- å“ªäº›å˜é‡å°†ä¼ é€’åˆ°æç¤ºè¯æ¨¡æ¿ä¸­
- è¾“å‡ºä¸åº”è¯¥åšä»€ä¹ˆçš„ä»»ä½•çº¦æŸ
- è¾“å‡ºå¿…é¡»éµå®ˆçš„ä»»ä½•è¦æ±‚

å¦‚æœä½ æ— æ³•è¾¨åˆ«è¿™äº›ä¿¡æ¯ï¼Œè¯·è¦æ±‚ç”¨æˆ·æ¾„æ¸…ï¼ä¸è¦å°è¯•éšæ„çŒœæµ‹ã€‚

åœ¨ä½ èƒ½å¤Ÿè¾¨åˆ«æ‰€æœ‰ä¿¡æ¯åï¼Œè°ƒç”¨ç›¸å…³å·¥å…·ã€‚"""

prompt_system = """æ ¹æ®ä»¥ä¸‹éœ€æ±‚ï¼Œç¼–å†™ä¸€ä¸ªä¼˜è´¨çš„æç¤ºè¯æ¨¡æ¿ï¼š

{reqs}"""

# Define PromptInstructions class
class PromptInstructions(BaseModel):
    """Instructions on how to prompt the LLM."""
    objective: str
    variables: List[str]
    constraints: List[str]
    requirements: List[str]

# ç»‘å®šå·¥å…·åˆ°LLM
llm_with_tool = llm.bind_tools([PromptInstructions])

# å®šä¹‰æ¶ˆæ¯å¤„ç†å‡½æ•°
def get_messages_info(messages):
    # æ·»åŠ ç³»ç»Ÿæç¤ºå¹¶ä¿ç•™å†å²æ¶ˆæ¯
    return [SystemMessage(content=template)] + messages

def get_prompt_messages(messages: list):
    # æå–å·¥å…·è°ƒç”¨å‚æ•°å’Œç›¸å…³æ¶ˆæ¯
    tool_call = None
    other_msgs = []
    for m in messages:
        if isinstance(m, AIMessage) and m.tool_calls:
            # æå–å·¥å…·è°ƒç”¨çš„å‚æ•°ä¿¡æ¯
            tool_call = m.tool_calls[0]["args"]
        elif isinstance(m, ToolMessage):
            # è·³è¿‡å·¥å…·æ¶ˆæ¯
            continue
        elif tool_call is not None:
            # æ”¶é›†å·¥å…·è°ƒç”¨åçš„æ¶ˆæ¯
            other_msgs.append(m)
    
    # ä½¿ç”¨æå–çš„å‚æ•°æ„å»ºæç¤ºè¯ç³»ç»ŸæŒ‡ä»¤
    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs

# å®šä¹‰å·¥ä½œæµé“¾å‡½æ•°
def info_chain(state):
    # æ”¶é›†ç”¨æˆ·éœ€æ±‚å¹¶å¼•å¯¼ç”¨æˆ·æä¾›å¿…è¦ä¿¡æ¯
    messages = get_messages_info(state["messages"])
    # è°ƒç”¨å¸¦å·¥å…·çš„LLMç”Ÿæˆå›åº”æˆ–å·¥å…·è°ƒç”¨
    response = llm_with_tool.invoke(messages)
    return {"messages": [response]}

def prompt_gen_chain(state):
    # ç”Ÿæˆå®é™…çš„æç¤ºè¯æ¨¡æ¿
    messages = get_prompt_messages(state["messages"])
    # è°ƒç”¨LLMç”Ÿæˆæç¤ºè¯æ¨¡æ¿
    response = llm.invoke(messages)
    
    # æ ¼å¼åŒ–æç¤ºè¯æ¨¡æ¿å¹¶å¢åŠ ä½¿ç”¨è¯´æ˜
    formatted_content = response.content.strip()
    enhanced_content = f"### ğŸª„ ç”Ÿæˆçš„æç¤ºè¯æ¨¡æ¿\n\n```\n{formatted_content}\n```\n\næ‚¨å¯ä»¥å¤åˆ¶ä¸Šé¢çš„æç¤ºè¯å¹¶æ ¹æ®éœ€è¦è¿›è¡Œè°ƒæ•´ã€‚\n\nå¦‚æœæ‚¨å¯¹è¿™ä¸ªæç¤ºè¯æ¨¡æ¿æœ‰ä»»ä½•ç–‘é—®æˆ–éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼"
    
    # åˆ›å»ºå¸¦æœ‰å¢å¼ºæ ¼å¼çš„æ–°å“åº”æ¶ˆæ¯
    enhanced_response = AIMessage(content=enhanced_content)
    return {"messages": [enhanced_response]}

# Define state handler
def get_state(state):
    messages = state["messages"]
    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:
        return "add_tool_message"
    elif not isinstance(messages[-1], HumanMessage):
        return END
    return "info"

# Define State type
class State(TypedDict):
    messages: Annotated[list, add_messages]

# Create workflow
memory = MemorySaver()
workflow = StateGraph(State)

# Add nodes to workflow
workflow.add_node("info", info_chain)
workflow.add_node("prompt", prompt_gen_chain)

@workflow.add_node
def add_tool_message(state: State):
    # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
    tool_call = state["messages"][-1].tool_calls[0]
    tool_id = tool_call["id"]
    tool_args = tool_call["args"]
    
    # ä»å·¥å…·è°ƒç”¨ä¸­æå–éœ€æ±‚å†…å®¹ç”¨äºå±•ç¤º
    if isinstance(tool_args, dict):
        objective = tool_args.get("objective", "")
        variables = tool_args.get("variables", [])
        constraints = tool_args.get("constraints", [])
        requirements = tool_args.get("requirements", [])
        
        # æ„å»ºéœ€æ±‚æ¦‚è§ˆæ–‡æœ¬
        reqs_summary = f"### ğŸ“‹ éœ€æ±‚æ¦‚è§ˆ\n\n"
        reqs_summary += f"**ç›®æ ‡**: {objective}\n\n"
        
        if variables:
            reqs_summary += f"**å˜é‡**:\n"
            for var in variables:
                reqs_summary += f"- {var}\n"
            reqs_summary += "\n"
            
        if constraints:
            reqs_summary += f"**çº¦æŸ**:\n"
            for constraint in constraints:
                reqs_summary += f"- {constraint}\n"
            reqs_summary += "\n"
            
        if requirements:
            reqs_summary += f"**è¦æ±‚**:\n"
            for req in requirements:
                reqs_summary += f"- {req}\n"
                
        message_content = f"{reqs_summary}\n\nâ³ æ­£åœ¨æ ¹æ®æ‚¨çš„éœ€æ±‚ç”Ÿæˆæç¤ºè¯æ¨¡æ¿..."
    else:
        # å¤‡ç”¨æ¶ˆæ¯ï¼Œå¦‚æœæ²¡æœ‰æ­£ç¡®æå–åˆ°å‚æ•°
        message_content = "â³ æ­£åœ¨ç”Ÿæˆæç¤ºè¯æ¨¡æ¿..."  
        
    return {
        "messages": [
            ToolMessage(
                content=message_content,
                tool_call_id=tool_id,
            )
        ]
    }

# Add edges to workflow
workflow.add_conditional_edges("info", get_state, ["add_tool_message", "info", END])
workflow.add_edge("add_tool_message", "prompt")
workflow.add_edge("prompt", END)
workflow.add_edge(START, "info")

# Compile workflow
graph = workflow.compile(checkpointer=memory)

# Message history and thread management
class ChatManager:
    def __init__(self):
        self.thread_id = str(uuid.uuid4())
        self.config = {"configurable": {"thread_id": self.thread_id, "verbose": True}}
        self.message_history = []
        
    def reset(self):
        """Reset the chat history and create a new thread."""
        self.thread_id = str(uuid.uuid4())
        self.config = {"configurable": {"thread_id": self.thread_id, "verbose": True}}
        self.message_history = []

# Initialize the chat manager
chat_manager = ChatManager()

# å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶å®ç°æµå¼è¾“å‡º
def process_message(message: str, history: List[List[str]]) -> Generator[str, None, None]:
    """
    å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶æµå¼è¾“å‡ºå“åº”ã€‚
    
    å‚æ•°:
        message: ç”¨æˆ·æ¶ˆæ¯
        history: Gradioæ ¼å¼çš„èŠå¤©å†å²
        
    è¿”å›:
        ç”Ÿæˆå™¨ï¼Œæµå¼è¾“å‡ºå“åº”
    """
    logger.info(f"Processing user message: {message[:50]}...")
    
    try:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†…éƒ¨å†å²è®°å½•
        chat_manager.message_history.append(HumanMessage(content=message))
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        input_data = {"messages": chat_manager.message_history}
        
        # åˆå§‹åŒ–è¾“å‡ºå˜é‡
        output = ""
        waiting_messages = []  # ç­‰å¾…æ¶ˆæ¯åˆ—è¡¨ï¼Œç”¨äºæ˜¾ç¤ºå·¥å…·è°ƒç”¨çŠ¶æ€
        
        # ç¡®ä¿è‡³å°‘ç”Ÿæˆä¸€ä¸ªåˆå§‹å“åº”ï¼Œé¿å…ç”Ÿæˆå™¨è¿‡æ—©ç»“æŸ
        yield "æ­£åœ¨æ€è€ƒ..."
        
        # æ—¥å¿—è®°å½•å¼€å§‹æ‹¿å–æµå¼è¾“å‡º
        logger.info("Start streaming LangGraph response")
        
        # ä½¿ç”¨ä¸€èˆ¬æ¨¡å¼è·å–è¿”å›å†…å®¹ï¼Œæ›´åŠ ç®€å•ç¨³å®š
        for chunk in graph.stream(input_data, config=chat_manager.config):
            # è·å–æ¶ˆæ¯åˆ—è¡¨
            messages = next(iter(chunk.values())).get("messages", [])
            if not messages:
                logger.warning("Received empty messages from graph")
                continue
                
            logger.info(f"Received {len(messages)} messages from graph")
            
            # è·å–æœ€æ–°æ¶ˆæ¯
            latest_message = messages[-1]
            
            # å¤„ç†AIæ¶ˆæ¯
            if isinstance(latest_message, AIMessage):
                if latest_message.content:
                    # å°†æ¶ˆæ¯æ·»åŠ åˆ°å†å²è®°å½•
                    if latest_message not in chat_manager.message_history:
                        chat_manager.message_history.append(latest_message)
                    
                    # æ£€æµ‹æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                    if hasattr(latest_message, "tool_calls") and latest_message.tool_calls:
                        logger.info("Detected tool calls in AI message")
                        # æ˜¾ç¤ºå·¥å…·è°ƒç”¨çŠ¶æ€
                        output = f"{latest_message.content}\n\næ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚..."
                    else:
                        # æ›´æ–°å“åº”
                        output = latest_message.content
                    
                    # æµå¼è¾“å‡ºå½“å‰å“åº”
                    logger.info(f"Yielding AI response: {output[:50]}...")
                    yield output
            
            # å¤„ç†å·¥å…·æ¶ˆæ¯
            elif isinstance(latest_message, ToolMessage):
                # å°†æ¶ˆæ¯æ·»åŠ åˆ°å†å²è®°å½•
                if latest_message not in chat_manager.message_history:
                    chat_manager.message_history.append(latest_message)
                
                # æ˜¾ç¤ºå·¥å…·æ¶ˆæ¯çŠ¶æ€
                logger.info("Received tool message (prompt template)")
                if "\n\næ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚..." in output:
                    # å¦‚æœä¹‹å‰æœ‰åˆ†æçŠ¶æ€ï¼Œå»æ‰å®ƒ
                    base_content = output.split("\n\næ­£åœ¨åˆ†ææ‚¨çš„éœ€æ±‚...")[0]
                    output = f"{base_content}\n\næç¤ºè¯æ¨¡æ¿ç”Ÿæˆå®Œæˆ!\n\n{latest_message.content}"
                else:
                    # ç›´æ¥è®¾ç½®å·¥å…·æ¶ˆæ¯å†…å®¹
                    output = latest_message.content
                
                logger.info(f"Yielding tool response: {output[:50]}...")
                yield output
        
        # ç¡®ä¿è¿”å›æœ€ç»ˆå“åº”
        logger.info("Streaming completed, returning final response")
        return output
        
    except Exception as e:
        # ä¾‹å¤–å¤„ç†
        error_msg = f"éå¸¸æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶å‡ºç°é”™è¯¯: {str(e)}"
        logger.exception("Error in process_message:")
        yield error_msg
        return error_msg

# é‡ç½®èŠå¤©å†å²çš„å‡½æ•°
def reset_chat():
    chat_manager.reset()
    return None

# ä½¿ç”¨ChatInterfaceå®šä¹‰Gradioåº”ç”¨
demo = gr.ChatInterface(
    fn=process_message,
    title="ğŸª„ Magic-Prompt æç¤ºè¯ç”Ÿæˆå™¨",
    description="AIé©±åŠ¨çš„é«˜è´¨é‡æç¤ºè¯æ¨¡æ¿ç”Ÿæˆå·¥å…·ã€‚æè¿°æ‚¨æƒ³è¦åˆ›å»ºçš„æç¤ºè¯ç±»å‹ï¼Œæˆ‘å°†æŒ‡å¯¼æ‚¨å®Œæˆåˆ›å»ºè¿‡ç¨‹ã€‚",
    examples=[
        "æˆ‘éœ€è¦ä¸€ä¸ªç”¨äºåˆ›æ„å†™ä½œçš„æç¤ºè¯", 
        "å¸®æˆ‘åˆ›å»ºä¸€ä¸ªRAGç³»ç»Ÿçš„æç¤ºè¯", 
        "è®¾è®¡ä¸€ä¸ªå›¾åƒç”Ÿæˆçš„æç¤ºè¯",
        "å†™ä¸€ä¸ªèƒ½æ€»ç»“æ–‡ç« çš„æç¤ºè¯æ¨¡æ¿"
    ],
    theme="soft",
    type="messages",
    fill_height=True,
    chatbot=gr.Chatbot(
        height=650, 
        type="messages",
        show_copy_button=True
    ),
    save_history=True
)

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    print("âœ¨ æ­£åœ¨å¯åŠ¨ Magic-Prompt æç¤ºè¯ç”Ÿæˆå™¨...")
    print("ğŸ’¡ ä½¿ç”¨æ–¹æ³•: æè¿°æ‚¨æƒ³è¦åˆ›å»ºçš„æç¤ºè¯ï¼ŒæŒ‰ç…§AIçš„å¼•å¯¼æä¾›ä¿¡æ¯")
    print("âš™ï¸ æŠ€æœ¯æ ˆ: LangChain + LangGraph + Gradio + OpenAI API")
    
    # å¯åŠ¨Gradioåº”ç”¨
    demo.launch(
        show_api=False,
        share=False,
        inbrowser=True,
        favicon_path="ğŸª„"
    )
